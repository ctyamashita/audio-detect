/**
 * Bundled by jsDelivr using Rollup v2.79.2 and Terser v5.39.0.
 * Original file: /npm/@ricky0123/vad-web@0.0.29/dist/index.js
 *
 * Do NOT use SRI with dynamically generated files! More information: https://www.jsdelivr.com/using-sri-with-dynamic-files
 */
import e from "/npm/onnxruntime-web@1.23.0/+esm"; import t from "/npm/onnxruntime-web@1.23.0/wasm/+esm"; var s = "undefined" != typeof globalThis ? globalThis : "undefined" != typeof window ? window : "undefined" != typeof global ? global : "undefined" != typeof self ? self : {}, r = {}, o = {}; Object.defineProperty(o, "__esModule", { value: !0 }), o.baseAssetPath = void 0; const i = "undefined" != typeof window && void 0 !== window.document ? window.document.currentScript : null; let a = "/"; i && (a = i.src.replace(/#.*$/, "").replace(/\?.*$/, "").replace(/\/[^/]+$/, "/")), o.baseAssetPath = a; var n = {}; Object.defineProperty(n, "__esModule", { value: !0 }), n.defaultModelFetcher = void 0; n.defaultModelFetcher = e => fetch(e).then((e => e.arrayBuffer())); var c = {}, h = {}; Object.defineProperty(h, "__esModule", { value: !0 }), h.log = void 0; const d = e => t => { console.log(`VAD | ${e} >`, t) }; h.log = { error: d("error"), debug: d("debug"), warn: d("warn") }; var l, u = {}; Object.defineProperty(u, "__esModule", { value: !0 }), u.Message = void 0, function (e) { e.AudioFrame = "AUDIO_FRAME", e.SpeechStart = "SPEECH_START", e.VADMisfire = "VAD_MISFIRE", e.SpeechEnd = "SPEECH_END", e.SpeechStop = "SPEECH_STOP", e.SpeechRealStart = "SPEECH_REAL_START", e.FrameProcessed = "FRAME_PROCESSED" }(l || (u.Message = l = {})), Object.defineProperty(c, "__esModule", { value: !0 }), c.FrameProcessor = c.validateOptions = c.defaultFrameProcessorOptions = void 0; const p = h, m = u; c.defaultFrameProcessorOptions = { positiveSpeechThreshold: .3, negativeSpeechThreshold: .25, preSpeechPadMs: 800, redemptionMs: 1400, minSpeechMs: 400, submitUserSpeechOnPause: !1 }, c.validateOptions = function (e) { (e.positiveSpeechThreshold < 0 || e.positiveSpeechThreshold > 1) && p.log.error("positiveSpeechThreshold should be a number between 0 and 1"), (e.negativeSpeechThreshold < 0 || e.negativeSpeechThreshold > e.positiveSpeechThreshold) && p.log.error("negativeSpeechThreshold should be between 0 and positiveSpeechThreshold"), e.preSpeechPadMs < 0 && p.log.error("preSpeechPadMs should be positive"), e.redemptionMs < 0 && p.log.error("redemptionMs should be positive"), e.minSpeechMs < 0 && p.log.error("minSpeechMs should be positive") }; const f = e => { const t = e.reduce(((e, t) => (e.push(e.at(-1) + t.length), e)), [0]), s = new Float32Array(t.at(-1)); return e.forEach(((e, r) => { const o = t[r]; s.set(e, o) })), s }; function g(e, t) { return { redemptionFrames: Math.floor(e.redemptionMs / t), preSpeechPadFrames: Math.floor(e.preSpeechPadMs / t), minSpeechFrames: Math.floor(e.minSpeechMs / t) } } c.FrameProcessor = class { constructor(e, t, s, r) { this.modelProcessFunc = e, this.modelResetFunc = t, this.options = s, this.msPerFrame = r, this.speaking = !1, this.redemptionCounter = 0, this.speechFrameCount = 0, this.active = !1, this.speechRealStartFired = !1, this.setOptions = e => { this.options = { ...this.options, ...e }; const { redemptionFrames: t, preSpeechPadFrames: s, minSpeechFrames: r } = g(this.options, this.msPerFrame); this.redemptionFrames = t, this.preSpeechPadFrames = s, this.minSpeechFrames = r }, this.reset = () => { this.speaking = !1, this.speechRealStartFired = !1, this.audioBuffer = [], this.modelResetFunc(), this.redemptionCounter = 0, this.speechFrameCount = 0 }, this.pause = e => { this.active = !1, this.options.submitUserSpeechOnPause ? this.endSegment(e) : this.reset() }, this.resume = () => { this.active = !0 }, this.endSegment = e => { const t = this.audioBuffer; this.audioBuffer = []; const s = this.speaking; if (this.reset(), s) { if (t.reduce(((e, t) => t.isSpeech ? e + 1 : e), 0) >= this.minSpeechFrames) { const s = f(t.map((e => e.frame))); e({ msg: m.Message.SpeechEnd, audio: s }) } else e({ msg: m.Message.VADMisfire }) } return {} }, this.process = async (e, t) => { if (!this.active) return; const s = await this.modelProcessFunc(e), r = s.isSpeech >= this.options.positiveSpeechThreshold; if (t({ probs: s, msg: m.Message.FrameProcessed, frame: e }), this.audioBuffer.push({ frame: e, isSpeech: r }), r && (this.speechFrameCount++, this.redemptionCounter = 0), r && !this.speaking && (this.speaking = !0, t({ msg: m.Message.SpeechStart })), this.speaking && this.speechFrameCount === this.minSpeechFrames && !this.speechRealStartFired && (this.speechRealStartFired = !0, t({ msg: m.Message.SpeechRealStart })), s.isSpeech < this.options.negativeSpeechThreshold && this.speaking && ++this.redemptionCounter >= this.redemptionFrames) { this.redemptionCounter = 0, this.speechFrameCount = 0, this.speaking = !1, this.speechRealStartFired = !1; const e = this.audioBuffer; this.audioBuffer = []; if (e.reduce(((e, t) => t.isSpeech ? e + 1 : e), 0) >= this.minSpeechFrames) { const s = f(e.map((e => e.frame))); t({ msg: m.Message.SpeechEnd, audio: s }) } else t({ msg: m.Message.VADMisfire }) } if (!this.speaking) { for (; this.audioBuffer.length > this.preSpeechPadFrames;)this.audioBuffer.shift(); this.speechFrameCount = 0 } }, this.audioBuffer = []; const { redemptionFrames: o, preSpeechPadFrames: i, minSpeechFrames: a } = g(this.options, this.msPerFrame); this.redemptionFrames = o, this.preSpeechPadFrames = i, this.minSpeechFrames = a, this.reset() } }; var S = {}, v = {}, _ = {}; Object.defineProperty(_, "__esModule", { value: !0 }); var w, b = {}; Object.defineProperty(b, "__esModule", { value: !0 }), b.SileroLegacy = void 0; const y = h; class M { constructor(e, t, s, r, o) { this.ortInstance = e, this._session = t, this._h = s, this._c = r, this._sr = o, this.reset_state = () => { const e = Array(128).fill(0); this._h = new this.ortInstance.Tensor("float32", e, [2, 1, 64]), this._c = new this.ortInstance.Tensor("float32", e, [2, 1, 64]) }, this.process = async e => { const t = { input: new this.ortInstance.Tensor("float32", e, [1, e.length]), h: this._h, c: this._c, sr: this._sr }, s = await this._session.run(t); this._h = s.hn, this._c = s.cn; const [r] = s.output?.data; return { notSpeech: 1 - r, isSpeech: r } } } } b.SileroLegacy = M, w = M, M.new = async (e, t) => { y.log.debug("initializing vad"); const s = await t(), r = await e.InferenceSession.create(s), o = new e.Tensor("int64", [16000n]), i = Array(128).fill(0), a = new e.Tensor("float32", i, [2, 1, 64]), n = new e.Tensor("float32", i, [2, 1, 64]); y.log.debug("vad is initialized"); return new w(e, r, a, n, o) }; var A, F = {}; Object.defineProperty(F, "__esModule", { value: !0 }), F.SileroV5 = void 0; const P = h; function O(e) { const t = Array(256).fill(0); return new e.Tensor("float32", t, [2, 1, 128]) } class T { constructor(e, t, s, r) { this._session = e, this._state = t, this._sr = s, this.ortInstance = r, this.reset_state = () => { this._state = O(this.ortInstance) }, this.process = async e => { const t = { input: new this.ortInstance.Tensor("float32", e, [1, e.length]), state: this._state, sr: this._sr }, s = await this._session.run(t); if (!s.stateN) throw new Error("No state from model"); if (this._state = s.stateN, !s.output?.data) throw new Error("No output from model"); const [r] = s.output?.data; return { notSpeech: 1 - r, isSpeech: r } } } } F.SileroV5 = T, A = T, T.new = async (e, t) => { P.log.debug("Loading VAD..."); const s = await t(), r = await e.InferenceSession.create(s), o = new e.Tensor("int64", [16000n]), i = O(e); return P.log.debug("...finished loading VAD"), new A(r, i, o, e) }, function (e) { var t = s && s.__createBinding || (Object.create ? function (e, t, s, r) { void 0 === r && (r = s); var o = Object.getOwnPropertyDescriptor(t, s); o && !("get" in o ? !t.__esModule : o.writable || o.configurable) || (o = { enumerable: !0, get: function () { return t[s] } }), Object.defineProperty(e, r, o) } : function (e, t, s, r) { void 0 === r && (r = s), e[r] = t[s] }), r = s && s.__exportStar || function (e, s) { for (var r in e) "default" === r || Object.prototype.hasOwnProperty.call(s, r) || t(s, e, r) }; Object.defineProperty(e, "__esModule", { value: !0 }), e.SileroV5 = e.SileroLegacy = void 0, r(_, e); var o = b; Object.defineProperty(e, "SileroLegacy", { enumerable: !0, get: function () { return o.SileroLegacy } }); var i = F; Object.defineProperty(e, "SileroV5", { enumerable: !0, get: function () { return i.SileroV5 } }) }(v); var D = {}; Object.defineProperty(D, "__esModule", { value: !0 }), D.Resampler = void 0; const E = h; D.Resampler = class { constructor(e) { this.options = e, this.process = e => { const t = []; for (const s of e) for (this.inputBuffer.push(s); this.hasEnoughDataForFrame();) { const e = this.generateOutputFrame(); t.push(e) } return t }, e.nativeSampleRate < 16e3 && E.log.error("nativeSampleRate is too low. Should have 16000 = targetSampleRate <= nativeSampleRate"), this.inputBuffer = [] } async*stream(e) { for (const t of e) for (this.inputBuffer.push(t); this.hasEnoughDataForFrame();) { const e = this.generateOutputFrame(); yield e } } hasEnoughDataForFrame() { return this.inputBuffer.length * this.options.targetSampleRate / this.options.nativeSampleRate >= this.options.targetFrameSize } generateOutputFrame() { const e = new Float32Array(this.options.targetFrameSize); let t = 0, s = 0; for (; t < this.options.targetFrameSize;) { let r = 0, o = 0; for (; s < Math.min(this.inputBuffer.length, (t + 1) * this.options.nativeSampleRate / this.options.targetSampleRate);) { const e = this.inputBuffer[s]; void 0 !== e && (r += e, o++), s++ } e[t] = r / o, t++ } return this.inputBuffer = this.inputBuffer.slice(s), e } }, function (t) { var r = s && s.__createBinding || (Object.create ? function (e, t, s, r) { void 0 === r && (r = s); var o = Object.getOwnPropertyDescriptor(t, s); o && !("get" in o ? !t.__esModule : o.writable || o.configurable) || (o = { enumerable: !0, get: function () { return t[s] } }), Object.defineProperty(e, r, o) } : function (e, t, s, r) { void 0 === r && (r = s), e[r] = t[s] }), i = s && s.__setModuleDefault || (Object.create ? function (e, t) { Object.defineProperty(e, "default", { enumerable: !0, value: t }) } : function (e, t) { e.default = t }), a = s && s.__importStar || function (e) { if (e && e.__esModule) return e; var t = {}; if (null != e) for (var s in e) "default" !== s && Object.prototype.hasOwnProperty.call(e, s) && r(t, e, s); return i(t, e), t }; Object.defineProperty(t, "__esModule", { value: !0 }), t.NonRealTimeVAD = t.defaultNonRealTimeVADOptions = void 0; const h = a(e), d = o, l = n, p = c, m = u, f = v, g = D; t.defaultNonRealTimeVADOptions = { ...p.defaultFrameProcessorOptions, modelURL: d.baseAssetPath + "silero_vad_legacy.onnx", modelFetcher: l.defaultModelFetcher }; t.NonRealTimeVAD = class { static async new(e = {}) { const s = { ...t.defaultNonRealTimeVADOptions, ...e }; (0, p.validateOptions)(s), void 0 !== s.ortConfig && s.ortConfig(h); const r = () => s.modelFetcher(s.modelURL), o = await f.SileroLegacy.new(h, r), i = new p.FrameProcessor(o.process, o.reset_state, { positiveSpeechThreshold: s.positiveSpeechThreshold, negativeSpeechThreshold: s.negativeSpeechThreshold, redemptionMs: s.redemptionMs, preSpeechPadMs: s.preSpeechPadMs, minSpeechMs: s.minSpeechMs, submitUserSpeechOnPause: s.submitUserSpeechOnPause }, 96); i.resume(); return new this(r, h, s, i) } constructor(e, t, s, r) { this.modelFetcher = e, this.ort = t, this.options = s, this.frameProcessor = r, this.frameSamples = 1536 } async*run(e, t) { const s = { nativeSampleRate: t, targetSampleRate: 16e3, targetFrameSize: this.frameSamples }, r = new g.Resampler(s); let o = 0, i = 0, a = 0; for await (const t of r.stream(e)) { const e = []; await this.frameProcessor.process(t, (t => { e.push(t) })); for (const t of e) switch (t.msg) { case m.Message.SpeechStart: o = a * this.frameSamples / 16; break; case m.Message.SpeechEnd: i = (a + 1) * this.frameSamples / 16, yield { audio: t.audio, start: o, end: i } }a++ } const n = []; this.frameProcessor.endSegment((e => { n.push(e) })); for (const e of n) if (e.msg === m.Message.SpeechEnd) yield { audio: e.audio, start: o, end: a * this.frameSamples / 16 } } } }(S); var R = {}; function C(e, t, s) { for (let r = 0; r < s.length; r++)e.setUint8(t + r, s.charCodeAt(r)) } Object.defineProperty(R, "__esModule", { value: !0 }), R.audioFileToArray = R.encodeWAV = R.arrayBufferToBase64 = R.minFramesForTargetMS = void 0, R.minFramesForTargetMS = function (e, t, s = 16e3) { return Math.ceil(e * s / 1e3 / t) }, R.arrayBufferToBase64 = function (e) { const t = new Uint8Array(e), s = t.byteLength, r = new Array(s); for (let e = 0; e < s; e++) { const s = t[e]; if (void 0 === s) break; r[e] = String.fromCharCode(s) } return btoa(r.join("")) }, R.encodeWAV = function (e, t = 3, s = 16e3, r = 1, o = 32) { const i = o / 8, a = r * i, n = new ArrayBuffer(44 + e.length * i), c = new DataView(n); return C(c, 0, "RIFF"), c.setUint32(4, 36 + e.length * i, !0), C(c, 8, "WAVE"), C(c, 12, "fmt "), c.setUint32(16, 16, !0), c.setUint16(20, t, !0), c.setUint16(22, r, !0), c.setUint32(24, s, !0), c.setUint32(28, s * a, !0), c.setUint16(32, a, !0), c.setUint16(34, o, !0), C(c, 36, "data"), c.setUint32(40, e.length * i, !0), 1 === t ? function (e, t, s) { for (let r = 0; r < s.length; r++, t += 2) { const o = Math.max(-1, Math.min(1, s[r])); e.setInt16(t, o < 0 ? 32768 * o : 32767 * o, !0) } }(c, 44, e) : function (e, t, s) { for (let r = 0; r < s.length; r++, t += 4)e.setFloat32(t, s[r], !0) }(c, 44, e), n }, R.audioFileToArray = async function (e) { const t = new OfflineAudioContext(1, 1, 44100), s = new FileReader; let r = null; if (await new Promise((o => { s.addEventListener("loadend", (() => { const e = s.result; t.decodeAudioData(e, (e => { r = e, t.startRendering().then((() => { console.log("Rendering completed successfully"), o() })).catch((e => { console.error(`Rendering failed: ${e}`) })) }), (e => { console.log(`Error with decoding audio data: ${e}`) })) })), s.readAsArrayBuffer(e) })), null === r) throw Error("some shit"); const o = r, i = new Float32Array(o.length); for (let e = 0; e < o.length; e++)for (let t = 0; t < o.numberOfChannels; t++) { const s = o.getChannelData(t)[e], r = i[e]; if (void 0 === s || void 0 === r) throw new Error("sample or out[i] is undefined"); i[e] = r + s } return { audio: i, sampleRate: o.sampleRate } }; var V = {}; !function (e) { var r = s && s.__createBinding || (Object.create ? function (e, t, s, r) { void 0 === r && (r = s); var o = Object.getOwnPropertyDescriptor(t, s); o && !("get" in o ? !t.__esModule : o.writable || o.configurable) || (o = { enumerable: !0, get: function () { return t[s] } }), Object.defineProperty(e, r, o) } : function (e, t, s, r) { void 0 === r && (r = s), e[r] = t[s] }), o = s && s.__setModuleDefault || (Object.create ? function (e, t) { Object.defineProperty(e, "default", { enumerable: !0, value: t }) } : function (e, t) { e.default = t }), i = s && s.__importStar || function (e) { if (e && e.__esModule) return e; var t = {}; if (null != e) for (var s in e) "default" !== s && Object.prototype.hasOwnProperty.call(e, s) && r(t, e, s); return o(t, e), t }; Object.defineProperty(e, "__esModule", { value: !0 }), e.MicVAD = e.getDefaultRealTimeVADOptions = e.ort = e.DEFAULT_MODEL = void 0; const a = i(t), d = n, l = c, p = h, m = u, f = v, g = D; e.DEFAULT_MODEL = "legacy", e.ort = a; e.getDefaultRealTimeVADOptions = e => ({ ...l.defaultFrameProcessorOptions, onFrameProcessed: () => { }, onVADMisfire: () => { p.log.debug("VAD misfire") }, onSpeechStart: () => { p.log.debug("Detected speech start") }, onSpeechEnd: () => { p.log.debug("Detected speech end") }, onSpeechRealStart: () => { p.log.debug("Detected real speech start") }, baseAssetPath: "./", onnxWASMBasePath: "./", model: e, workletOptions: {}, getStream: async () => await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, echoCancellation: !0, autoGainControl: !0, noiseSuppression: !0 } }), pauseStream: async e => { e.getTracks().forEach((e => { e.stop() })) }, resumeStream: async () => await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, echoCancellation: !0, autoGainControl: !0, noiseSuppression: !0 } }), ortConfig: e => { e.env.logLevel = "error" }, startOnLoad: !0, processorType: "auto" }); class S { constructor(e, t, s, r = !1, o = null, i = null, a = null, n = null, c = null, h = null, d = "uninitialized", l = !1) { this.options = e, this.frameProcessor = t, this.frameSamples = s, this.listening = r, this.errored = o, this._stream = i, this._audioContext = a, this._vadNode = n, this._mediaStreamAudioSourceNode = c, this._audioProcessorAdapterType = h, this.initializationState = d, this.ownsAudioContext = l, this.getAudioInstances = () => { if (null === this._stream || null === this._audioContext || null == this._vadNode || null == this._mediaStreamAudioSourceNode) throw new Error("MicVAD has null stream, audio context, or processor adapter"); return { stream: this._stream, audioContext: this._audioContext, vadNode: this._vadNode, mediaStreamAudioSourceNode: this._mediaStreamAudioSourceNode } }, this.setErrored = e => { this.initializationState = "errored", this.errored = e }, this.start = async () => { switch (this.initializationState) { case "uninitialized": p.log.debug("initializing micVAD"), this.initializationState = "initializing", this.frameProcessor.resume(); try { this._stream = await this.options.getStream() } catch (e) { throw e instanceof Error ? this.setErrored(e.message) : this.setErrored(String(e)), e } if (this.options.audioContext || (this._audioContext = new AudioContext, this.ownsAudioContext = !0), !this._audioContext) throw this.setErrored("Audio context is null"), Error("Audio context is null"); switch (this._audioProcessorAdapterType = "auto" == this.options.processorType ? "audioWorklet" in this._audioContext && "function" == typeof AudioWorkletNode ? "AudioWorklet" : "ScriptProcessor" : this.options.processorType, this._audioProcessorAdapterType) { case "AudioWorklet": this._vadNode = await async function (e, t, s, r, o) { await s.audioWorklet.addModule(e), t.processorOptions = { ...t.processorOptions ?? {}, frameSamples: r }; const i = new AudioWorkletNode(s, "vad-helper-worklet", t); return i.port.onmessage = async e => { switch (e.data?.message) { case m.Message.AudioFrame: { let t = e.data.data; t instanceof ArrayBuffer || (t = new ArrayBuffer(e.data.data.byteLength), new Uint8Array(t).set(new Uint8Array(e.data.data))); const s = new Float32Array(t); await o(s); break } } }, i }(this.options.baseAssetPath + "vad.worklet.bundle.min.js", this.options.workletOptions ?? {}, this._audioContext, this.frameSamples, this.processFrame); break; case "ScriptProcessor": this._vadNode = await async function (e, t, s) { const r = new g.Resampler({ nativeSampleRate: e.sampleRate, targetSampleRate: 16e3, targetFrameSize: t ?? 480 }); p.log.debug("using script processor"); const o = e.createScriptProcessor(4096, 1, 1); let i = !1; return o.onaudioprocess = async e => { if (!i) { i = !0; try { const t = e.inputBuffer.getChannelData(0); if (e.outputBuffer.getChannelData(0).fill(0), r) { const e = r.process(t); for (const t of e) await s(t) } } catch (e) { console.error("Error processing audio:", e) } finally { i = !1 } } }, o.connect(e.destination), o }(this._audioContext, this.frameSamples, this.processFrame); break; default: throw new Error(`Unsupported audio processor adapter type: ${this._audioProcessorAdapterType}`) }this._mediaStreamAudioSourceNode = new MediaStreamAudioSourceNode(this._audioContext, { mediaStream: this._stream }), this._mediaStreamAudioSourceNode.connect(this._vadNode), p.log.debug("started micVAD"), this.listening = !0, this.initializationState = "initialized"; break; case "initializing": p.log.warn("start called while initializing"); break; case "initialized": { if (this.listening) return; this.listening = !0, this.frameProcessor.resume(); const { stream: e, audioContext: t, vadNode: s } = this.getAudioInstances(); this._stream = await this.options.resumeStream(e); const r = new MediaStreamAudioSourceNode(t, { mediaStream: this._stream }); this._mediaStreamAudioSourceNode = r, r.connect(s); break } case "destroyed": p.log.warn("start called after destroyed"); break; case "errored": p.log.error("start called after errored"); break; default: p.log.warn("weird initialization state") } }, this.pause = async () => { if (!this.listening) return; this.listening = !1; const { stream: e, mediaStreamAudioSourceNode: t } = this.getAudioInstances(); await this.options.pauseStream(e), t.disconnect(), this.frameProcessor.pause(this.handleFrameProcessorEvent) }, this.destroy = () => { p.log.debug("destroy called"), this.initializationState = "destroyed"; const { vadNode: e } = this.getAudioInstances(); e instanceof AudioWorkletNode && e.port.postMessage(m.Message.SpeechStop), this.listening && this.pause(), this.ownsAudioContext && this._audioContext?.close() }, this.setOptions = e => { this.frameProcessor.setOptions(e) }, this.processFrame = async e => { await this.frameProcessor.process(e, this.handleFrameProcessorEvent) }, this.handleFrameProcessorEvent = e => { switch (e.msg) { case m.Message.FrameProcessed: this.options.onFrameProcessed(e.probs, e.frame); break; case m.Message.SpeechStart: this.options.onSpeechStart(); break; case m.Message.SpeechRealStart: this.options.onSpeechRealStart(); break; case m.Message.VADMisfire: this.options.onVADMisfire(); break; case m.Message.SpeechEnd: this.options.onSpeechEnd(e.audio) } } } static async new(t = {}) { const s = { ...(0, e.getDefaultRealTimeVADOptions)(t.model ?? e.DEFAULT_MODEL), ...t }; (0, l.validateOptions)(s), e.ort.env.wasm.wasmPaths = s.onnxWASMBasePath, void 0 !== s.ortConfig && s.ortConfig(e.ort); const r = "v5" === s.model ? "silero_vad_v5.onnx" : "silero_vad_legacy.onnx", o = s.baseAssetPath + r, i = "v5" === s.model ? f.SileroV5.new : f.SileroLegacy.new; let a; try { a = await i(e.ort, (() => (0, d.defaultModelFetcher)(o))) } catch (e) { throw console.error(`Encountered an error while loading model file ${o}`), e } const n = "v5" === s.model ? 512 : 1536, c = n / 16, h = new l.FrameProcessor(a.process, a.reset_state, { positiveSpeechThreshold: s.positiveSpeechThreshold, negativeSpeechThreshold: s.negativeSpeechThreshold, redemptionMs: s.redemptionMs, preSpeechPadMs: s.preSpeechPadMs, minSpeechMs: s.minSpeechMs, submitUserSpeechOnPause: s.submitUserSpeechOnPause }, c), u = new S(s, h, n); if (s.startOnLoad) try { await u.start() } catch (e) { throw console.error("Error starting micVad", e), e } return u } } e.MicVAD = S }(V), function (e) { Object.defineProperty(e, "__esModule", { value: !0 }), e.getDefaultRealTimeVADOptions = e.MicVAD = e.DEFAULT_MODEL = e.utils = e.NonRealTimeVAD = e.Message = e.FrameProcessor = e.defaultModelFetcher = e.baseAssetPath = void 0; var t = o; Object.defineProperty(e, "baseAssetPath", { enumerable: !0, get: function () { return t.baseAssetPath } }); var s = n; Object.defineProperty(e, "defaultModelFetcher", { enumerable: !0, get: function () { return s.defaultModelFetcher } }); var r = c; Object.defineProperty(e, "FrameProcessor", { enumerable: !0, get: function () { return r.FrameProcessor } }); var i = u; Object.defineProperty(e, "Message", { enumerable: !0, get: function () { return i.Message } }); var a = S; Object.defineProperty(e, "NonRealTimeVAD", { enumerable: !0, get: function () { return a.NonRealTimeVAD } }); const h = R; e.utils = { audioFileToArray: h.audioFileToArray, minFramesForTargetMS: h.minFramesForTargetMS, arrayBufferToBase64: h.arrayBufferToBase64, encodeWAV: h.encodeWAV }; var d = V; Object.defineProperty(e, "DEFAULT_MODEL", { enumerable: !0, get: function () { return d.DEFAULT_MODEL } }), Object.defineProperty(e, "MicVAD", { enumerable: !0, get: function () { return d.MicVAD } }), Object.defineProperty(e, "getDefaultRealTimeVADOptions", { enumerable: !0, get: function () { return d.getDefaultRealTimeVADOptions } }) }(r); var j = r.DEFAULT_MODEL, B = r.FrameProcessor, N = r.Message, k = r.MicVAD, U = r.NonRealTimeVAD, x = r.__esModule, L = r.baseAssetPath, z = r.defaultModelFetcher, I = r.getDefaultRealTimeVADOptions, W = r.utils; export { j as DEFAULT_MODEL, B as FrameProcessor, N as Message, k as MicVAD, U as NonRealTimeVAD, x as __esModule, L as baseAssetPath, r as default, z as defaultModelFetcher, I as getDefaultRealTimeVADOptions, W as utils };
//# sourceMappingURL=/sm/d236835212277452e1df6449984c404fedada0c7a4093e103c70ffea6a546f3c.map